---
lab:
  title: An√°lisis y depuraci√≥n de la aplicaci√≥n de IA generativa con seguimiento
  description: Obt√©n informaci√≥n sobre c√≥mo depurar la aplicaci√≥n de IA generativa mediante el seguimiento de su flujo de trabajo desde la entrada del usuario hasta la respuesta del modelo y el procesamiento posterior.
---

# An√°lisis y depuraci√≥n de la aplicaci√≥n de IA generativa con seguimiento

Este ejercicio dura aproximadamente¬†**30**¬†minutos.

> **Nota**: en este ejercicio se presupone que tienes alg√∫n conocimiento de Fundici√≥n de IA de Azure, por lo que algunas instrucciones son intencionadamente menos detalladas para fomentar la exploraci√≥n m√°s activa y el aprendizaje pr√°ctico.

## Introducci√≥n

En este ejercicio, ejecutar√°s un asistente de IA generativa de varios pasos que recomienda excursiones de senderismo y sugiere equipo de excursionismo. Usar√°s las caracter√≠sticas de seguimiento del SDK de inferencia de Azure AI para analizar c√≥mo se ejecuta la aplicaci√≥n e identificar los puntos de decisi√≥n clave tomados por el modelo y la l√≥gica circundante.

Interactuar√°s con un modelo implementado para simular el recorrido real del usuario, realizar un seguimiento de cada fase de la aplicaci√≥n desde la entrada de usuario a la respuesta del modelo al procesamiento posterior y ver√°s los datos de seguimiento en Fundici√≥n de IA de Azure. Esto te ayudar√° a comprender c√≥mo el seguimiento mejora la observabilidad, simplifica la depuraci√≥n y apoya la optimizaci√≥n del rendimiento de las aplicaciones de IA generativa.

## Configuraci√≥n del entorno

Para completar las tareas de este ejercicio, necesitas:

- Un centro de Fundici√≥n de IA de Azure,
- Un proyecto de Fundici√≥n de IA de Azure,
- Un modelo implementado (como GPT-4o),
- Un recurso de Application Insights conectado.

### Creaci√≥n de un centro y un proyecto de Fundici√≥n de IA de Azure

Para configurar r√°pidamente un centro y un proyecto, se proporcionan instrucciones sencillas para usar la interfaz de usuario del Portal de la Fundici√≥n de IA de Azure.

1. En un explorador web, abre el [Portal de la Fundici√≥n de IA de Azure](https://ai.azure.com) en `https://ai.azure.com` e inicia sesi√≥n con tus credenciales de Azure.
1. En la p√°gina principal, selecciona **+Crear proyecto**.
1. En el asistente para **crear un proyecto**, escribe un nombre v√°lido y si se te sugiere un centro existente, elige la opci√≥n para crear uno nuevo. A continuaci√≥n, revisa los recursos de Azure que se crear√°n autom√°ticamente para admitir el centro y el proyecto.
1. Selecciona **Personalizar** y especifica la siguiente configuraci√≥n para el centro:
    - **Nombre del centro**: *un nombre v√°lido para el centro*
    - **Suscripci√≥n**: *suscripci√≥n a Azure*
    - **Grupo de recursos**: *crea o selecciona un grupo de recursos*
    - **Ubicaci√≥n**: selecciona **Ayudarme a elegir** y, a continuaci√≥n, selecciona **gpt-4o** en la ventana Asistente de ubicaci√≥n y usa la regi√≥n recomendada\*
    - **Conectar Servicios de Azure AI o Azure OpenAI**: *crea un nuevo recurso de servicios de IA*
    - **Conectar B√∫squeda de Azure AI**: omite la conexi√≥n

    > \* Los recursos de Azure OpenAI est√°n restringidos por cuotas de modelo regionales. En caso de que se alcance un l√≠mite de cuota m√°s adelante en el ejercicio, es posible que tengas que crear otro recurso en otra regi√≥n.

1. Selecciona **Siguiente** y revisa tu configuraci√≥n. Luego, selecciona **Crear** y espera a que se complete el proceso.

### Implementaci√≥n de un modelo

Para generar datos que puedas supervisar, primero deber√°s implementar un modelo e interactuar con √©l. En las instrucciones se te pide que implementes un modelo GPT-4o, pero **puedes usar cualquier modelo** de la colecci√≥n de Azure OpenAI Service que est√© disponible.

1. En el men√∫ de la izquierda, en la secci√≥n **Mis recursos**, selecciona la p√°gina **Modelos y puntos de conexi√≥n**.
1. En el men√∫ **Implementar modelo**, selecciona **Implementar modelo base**.
1. Selecciona el modelo **gpt-4o** en la lista e implem√©ntalo con la siguiente configuraci√≥n; para ello, selecciona **Personalizar** en los detalles de implementaci√≥n:
    - **Nombre de implementaci√≥n**: *nombre v√°lido para la implementaci√≥n de modelo*
    - **Tipo de implementaci√≥n**: est√°ndar
    - **Actualizaci√≥n autom√°tica de la versi√≥n**: habilitado
    - **** Versi√≥n del modelo: *selecciona la versi√≥n disponible m√°s reciente*
    - **Recurso de IA conectado**: *selecciona tu conexi√≥n de recursos de Azure OpenAI*
    - **L√≠mite de frecuencia de tokens por minuto (miles)**: 5000
    - **Filtro de contenido**: DefaultV2
    - **Habilitaci√≥n de la cuota din√°mica**: deshabilitada

    > **Nota**: reducir el TPM ayuda a evitar el uso excesivo de la cuota disponible en la suscripci√≥n que est√° usando. 5000 TPM debe ser suficiente para los datos que se usan en este ejercicio. Si la cuota disponible es inferior a esta, podr√°s completar el ejercicio, pero se pueden producir errores si se supera el l√≠mite de velocidad.

1. Espera a que la implementaci√≥n se complete.

### Conectar Application Insights

Conecta Application Insights al proyecto en Fundici√≥n de IA de Azure para empezar a recopilar datos para su an√°lisis.

1. Usa el men√∫ de la izquierda y selecciona la p√°gina **Seguimiento**.
1. **Crea un nuevo** recurso de Application Insights para conectarse a la aplicaci√≥n.
1. Escribe un nombre de recurso de Application Insights y selecciona **Crear**.

Application Insights ahora est√° conectado al proyecto y los datos comenzar√°n a recopilarse para su an√°lisis.

## Ejecuci√≥n de una aplicaci√≥n de IA generativa con Cloud Shell

Te conectar√°s al proyecto de Fundici√≥n de IA de Azure desde Azure Cloud Shell e interactuar√°s mediante programaci√≥n con un modelo implementado como parte de una aplicaci√≥n de IA generativa.

### Interacci√≥n con un modelo implementado

Empieza por recuperar la informaci√≥n necesaria para autenticarte para interactuar con el modelo implementado. A continuaci√≥n, acceder√°s a Azure Cloud Shell y actualizar√°s el c√≥digo de la aplicaci√≥n de IA generativa.

1. En el Portal de la Fundici√≥n de IA de Azure, mira la p√°gina **Informaci√≥n general** del proyecto.
1. En el √°rea **Detalles del proyecto**, anota la **Cadena de conexi√≥n del proyecto**.
1. **Guarda** la cadena en un Bloc de notas. Usar√°s esta cadena de conexi√≥n para conectarte al proyecto en una aplicaci√≥n cliente.
1. Abre una nueva pesta√±a del explorador (mant√©n el Portal de la Fundici√≥n de IA de Azure abierto en la pesta√±a existente).
1. En la nueva pesta√±a, explora [Azure Portal](https://portal.azure.com) en `https://portal.azure.com` e inicia sesi√≥n con tus credenciales de Azure, si se te solicita.
1. Usa el bot√≥n **[\>_]** situado a la derecha de la barra de b√∫squeda en la parte superior de la p√°gina para crear una nueva instancia de Cloud Shell en Azure Portal, para lo que deber√°s seleccionar un entorno de ***PowerShell*** sin almacenamiento en tu suscripci√≥n.
1. En la barra de herramientas de Cloud Shell, en el men√∫ **Configuraci√≥n**, selecciona **Ir a la versi√≥n cl√°sica**.

    **<font color="red">Aseg√∫rate de que has cambiado a la versi√≥n cl√°sica de Cloud Shell antes de continuar.</font>**

1. En el panel de Cloud Shell, escribe y ejecuta los comandos siguientes:

    ```
    rm -r mslearn-genaiops -f
    git clone https://github.com/microsoftlearning/mslearn-genaiops mslearn-genaiops
    ```

    Este comando clona el repositorio de GitHub que contiene los archivos de c√≥digo de este ejercicio.

1. Una vez clonado el repo, ve a la carpeta que contiene los archivos de c√≥digo de aplicaci√≥n:  

    ```
   cd mslearn-genaiops/Files/08
    ```

1. En el panel de la l√≠nea de comandos de Cloud Shell, escribe el siguiente comando para instalar las bibliotecas que necesitas:

    ```
   python -m venv labenv
   ./labenv/bin/Activate.ps1
   pip install python-dotenv azure-identity azure-ai-projects azure-ai-inference azure-monitor-opentelemetry
    ```

1. Escribe el siguiente comando para abrir el archivo de configuraci√≥n que se ha proporcionado:

    ```
   code .env
    ```

    El archivo se abre en un editor de c√≥digo.

1. En el archivo de c√≥digo:

    1. Reemplaza el marcador de posici√≥n **your_project_connection_string** por la cadena de conexi√≥n del proyecto (copiado de la p√°gina **Informaci√≥n general** del proyecto en el Portal de la Fundici√≥n de IA de Azure).
    1. Reemplaza el marcador de posici√≥n **your_model_deployment** por el nombre que asignaste a la implementaci√≥n del modelo GPT-4o (de manera predeterminada `gpt-4o`).

1. *Despu√©s* de reemplazar el marcador de posici√≥n, en el editor de c√≥digo, usa el comando **CTRL+S** o **clic con el bot√≥n derecho > Guardar** para **guardar los cambios** y, a continuaci√≥n, usa el comando **CTRL+Q** o **clic con el bot√≥n derecho > Salir** para cerrar el editor de c√≥digo mientras mantienes abierta la l√≠nea de comandos de Cloud Shell.

### Actualizaci√≥n del c√≥digo de la aplicaci√≥n de IA generativa

Ahora que tanto el entorno como el archivo .env est√°n configurados, es el momento de preparar el script del asistente de IA para su ejecuci√≥n. Junto a la conexi√≥n con un proyecto de IA y la habilitaci√≥n de Application Insights, debes:

- Interactuar con el modelo implementado.
- Definir la funci√≥n para especificar la indicaci√≥n.
- Definir el flujo principal que llama a todas las funciones.

Agregar√° estas tres partes a un script inicial.

1. Ejecuta el siguiente comando para **abrir el script** que se ha proporcionado:

    ```
   code start-prompt.py
    ```

    Ver√°s que varias l√≠neas clave se han dejado en blanco o se han marcado con # comentarios vac√≠os. La tarea consiste en completar el script copiando y pegando las l√≠neas correctas a continuaci√≥n en las ubicaciones adecuadas.

1. En el script, busca **# Function to call the model and handle tracing**.
1. Pega el siguiente c√≥digo debajo de este comentario:

    ```
   def call_model(system_prompt, user_prompt, span_name):
        with tracer.start_as_current_span(span_name) as span:
            span.set_attribute("session.id", SESSION_ID)
            span.set_attribute("prompt.user", user_prompt)
            start_time = time.time()
    
            response = chat_client.complete(
                model=model_name,
                messages=[SystemMessage(system_prompt), UserMessage(user_prompt)]
            )
    
            duration = time.time() - start_time
            output = response.choices[0].message.content
            span.set_attribute("response.time", duration)
            span.set_attribute("response.tokens", len(output.split()))
            return output
    ```

1. En el script, busca **# Function to recommend a hike based on user preferences**.
1. Pega el siguiente c√≥digo debajo de este comentario:

    ```
   def recommend_hike(preferences):
        with tracer.start_as_current_span("recommend_hike") as span:
            prompt = f"""
            Recommend a named hiking trail based on the following user preferences.
            Provide only the name of the trail and a one-sentence summary.
            Preferences: {preferences}
            """
            response = call_model(
                "You are an expert hiking trail recommender.",
                prompt,
                "recommend_model_call"
            )
            span.set_attribute("hike_recommendation", response.strip())
            return response.strip()
    ```

1. En el script, busca **# ---- Main Flow ----**.
1. Pega el siguiente c√≥digo debajo de este comentario:

    ```
   if __name__ == "__main__":
       with tracer.start_as_current_span("trail_guide_session") as session_span:
           session_span.set_attribute("session.id", SESSION_ID)
           print("\n--- Trail Guide AI Assistant ---")
           preferences = input("Tell me what kind of hike you're looking for (location, difficulty, scenery):\n> ")

           hike = recommend_hike(preferences)
           print(f"\n‚úÖ Recommended Hike: {hike}")

           # Run profile function


           # Run match product function


           print(f"\nüîç Trace ID available in Application Insights for session: {SESSION_ID}")
    ```

1. **Guarda los cambios** realizados en el script.
1. En el panel de l√≠nea de comandos de Cloud Shell, debajo del editor de c√≥digo, escribe el siguiente comando para **ejecutar el script**:

    ```
   python start-prompt.py
    ```

1. Proporciona una descripci√≥n del tipo de caminata que est√°s buscando, por ejemplo:

    ```
   A one-day hike in the mountains
    ```

    El modelo generar√° una respuesta, que se capturar√° con Application Insights. Puedes visualizar los seguimientos en el **Portal de la Fundici√≥n de IA de Azure**.

> **Nota**: los datos de supervisi√≥n pueden tardar unos minutos en mostrarse en Azure Monitor.

## Visualizaci√≥n de los datos de seguimiento en el Portal de la Fundici√≥n de IA de Azure

Despu√©s de ejecutar el script, capturaste un seguimiento de la ejecuci√≥n de la aplicaci√≥n de IA. Ahora lo explorar√°s con Application Insights en Fundici√≥n de IA de Azure.

> **Nota:** m√°s adelante, volver√°s a ejecutar el c√≥digo y volver√°s a ver los seguimientos en el Portal de la Fundici√≥n de IA de Azure. Vamos a explorar primero d√≥nde encontrar los seguimientos para visualizarlos.

### Ve al Portal de la Fundici√≥n de IA de Azure

1. **¬°No cierres Cloud Shell!** Volver√°s a esto para actualizar el c√≥digo y volverlo a ejecutar.
1. Ve a la pesta√±a del explorador con el **Portal de la Fundici√≥n de IA de Azure** abierto.
1. Usa el men√∫ de la izquierda y selecciona **Seguimiento**.
1. *Si* no se muestra ning√∫n dato, **actualiza** la vista.
1. Selecciona el seguimiento **train_guide_session** para abrir una nueva ventana que muestre m√°s detalles.

### Revisi√≥n del seguimiento

En esta vista se muestra el seguimiento de una sesi√≥n completa del asistente de IA de la gu√≠a de senderos.

- **Intervalo de nivel superior**: trail_guide_session Este es el intervalo primario. Representa la ejecuci√≥n completa del asistente de principio a fin.

- **Intervalos secundarios anidados**: cada l√≠nea con sangr√≠a representa una operaci√≥n anidada. Encontrar√°s lo siguiente:

    - **recommend_hike** que captura la l√≥gica para decidir una caminata.
    - **recommend_model_call** que es el intervalo creado por call_model() dentro de recommend_hike.
    - **chat gpt-4o** que el SDK de inferencia de Azure AI instrumenta autom√°ticamente para mostrar la interacci√≥n real de LLM.

1. Puedes hacer clic en cualquier intervalo para ver:

    1. La duraci√≥n.
    1. Sus atributos, como la indicaci√≥n del usuario, los tokens usados, el tiempo de respuesta.
    1. Cualquier error o datos personalizados adjuntos con **span.set_attribute(...)**.

## Adici√≥n de m√°s funciones al c√≥digo

1. Ve a la pesta√±a del explorador con **Azure Portal** abierto.
1. Escribe el siguiente comando para **volver a abrir el script**:

    ```
   code start-prompt.py
    ```

1. En el script, busca **# Function to generate a trip profile for the recommended hike**.
1. Pega el siguiente c√≥digo debajo de este comentario:

    ```
   def generate_trip_profile(hike_name):
       with tracer.start_as_current_span("trip_profile_generation") as span:
           prompt = f"""
           Hike: {hike_name}
           Respond ONLY with a valid JSON object and nothing else.
           Do not include any intro text, commentary, or markdown formatting.
           Format: {{ "trailType": ..., "typicalWeather": ..., "recommendedGear": [ ... ] }}
           """
           response = call_model(
               "You are an AI assistant that returns structured hiking trip data in JSON format.",
               prompt,
               "trip_profile_model_call"
           )
           print("üîç Raw model response:", response)
           try:
               profile = json.loads(response)
               span.set_attribute("profile.success", True)
               return profile
           except json.JSONDecodeError as e:
               print("‚ùå JSON decode error:", e)
               span.set_attribute("profile.success", False)
               return {}
    ```

1. En el script, busca **# Function to match recommended gear with products in the catalog**.
1. Pega el siguiente c√≥digo debajo de este comentario:

    ```
   def match_products(recommended_gear):
       with tracer.start_as_current_span("product_matching") as span:
           matched = []
           for gear_item in recommended_gear:
               for product in mock_product_catalog:
                   if any(word in product.lower() for word in gear_item.lower().split()):
                       matched.append(product)
                       break
           span.set_attribute("matched.count", len(matched))
           return matched
    ```

1. En el script, busca **# Run profile function**.
1. Debajo y **alineado con** este comentario, pega el c√≥digo siguiente:

    ```
           profile = generate_trip_profile(hike)
           if not profile:
               print("Failed to generate trip profile. Please check Application Insights for trace.")
               exit(1)

           print(f"\nüìã Trip Profile for {hike}:")
           print(json.dumps(profile, indent=2))
    ```

1. En el script, busca **# Run match product function**.
1. Debajo y **alineado con** este comentario, pega el c√≥digo siguiente:

    ```
           matched = match_products(profile.get("recommendedGear", []))
           print("\nüõí Recommended Products from Lakeshore Retail:")
           print("\n".join(matched))
    ```

1. **Guarda los cambios** realizados en el script.
1. En el panel de l√≠nea de comandos de Cloud Shell, debajo del editor de c√≥digo, escribe el siguiente comando para **ejecutar el script**:

    ```
   python start-prompt.py
    ```

1. Proporciona una descripci√≥n del tipo de caminata que est√°s buscando, por ejemplo:

    ```
   I want to go for a multi-day adventure along the beach
    ```

<br>
<details>
<summary><b>Script de soluci√≥n</b>: en caso de que el c√≥digo no funcione.</summary><br>
<p>Si inspeccionas el seguimiento de LLM para la funci√≥n generate_trip_profile, observar√°s que la respuesta del asistente incluye acentos pendientes y la palabra json para dar formato a la salida como un bloque de c√≥digo.

Aunque esto resulta √∫til para mostrar, provoca problemas en el c√≥digo porque la salida ya no es JSON v√°lido. Esto produce un error de an√°lisis durante el procesamiento posterior.

Es probable que el error se deba a c√≥mo se indica a LLM que se ajuste a un formato espec√≠fico para su salida. La inclusi√≥n de las instrucciones en la indicaci√≥n del usuario aparece m√°s eficaz que colocarla en el aviso del sistema.</p>
</details>


> **Nota**: los datos de supervisi√≥n pueden tardar unos minutos en mostrarse en Azure Monitor.

### Visualizaci√≥n de los nuevos seguimientos en el Portal de la Fundici√≥n de IA de Azure

1. Vuelve al Portal de la Fundici√≥n de IA de Azure.
1. Deber√≠a aparecer un nuevo seguimiento con el mismo nombre **trail_guide_session**. Actualiza la vista si es necesario.
1. Selecciona el nuevo seguimiento para abrir una vista m√°s detallada.
1. Revisa los nuevos intervalos secundarios anidados **trip_profile_generation** y **product_matching**.
1. Selecciona **product_matching** y revisa los metadatos que aparecen.

    En la funci√≥n product_matching, inclu√≠ste **span.set_attribute("matched.count", len(matched)))**. Al establecer el atributo con el par clave-valor **matched.count** y la longitud de la variable coincidente, agregaste esta informaci√≥n al seguimiento de **product_matching**. Puedes encontrar este par clave-valor en **atributos** en los metadatos.

## (OPCIONAL) Seguimiento de un error

Si tienes tiempo adicional, puedes revisar c√≥mo usar seguimientos cuando tengas un error. Se proporciona un script que probablemente produzca un error. Ejec√∫talo y revisa los seguimientos.

Se trata de un ejercicio dise√±ado para representar un desaf√≠o, lo que significa que las instrucciones son intencionadamente menos detalladas.

1. En Cloud Shell, abre el script **error-prompt.py**. Este script se encuentra en el mismo directorio que el script **start-prompt.py**. Revisa el contenido.
1. Ejecuta el script **error-prompt.py**. Proporciona una respuesta en la l√≠nea de comandos cuando se te solicite.
1. *Esperamos* que el mensaje de salida incluya **No se pudo generar el perfil de viaje. Consulta Application Insights para el seguimiento.**.
1. Vaya al seguimiento de **trip_profile_generation** e inspecciona por qu√© se produjo un error.

<br>
<details>
<summary><b>Obtener la respuesta sobre</b>: ¬øPor qu√© puedes haber experimentado un error...</summary><br>
<p>Si inspeccionas el seguimiento de LLM para la funci√≥n generate_trip_profile, observar√°s que la respuesta del asistente incluye acentos pendientes y la palabra json para dar formato a la salida como un bloque de c√≥digo.

Aunque esto resulta √∫til para mostrar, provoca problemas en el c√≥digo porque la salida ya no es JSON v√°lido. Esto produce un error de an√°lisis durante el procesamiento posterior.

Es probable que el error se deba a c√≥mo se indica a LLM que se ajuste a un formato espec√≠fico para su salida. La inclusi√≥n de las instrucciones en la indicaci√≥n del usuario aparece m√°s eficaz que colocarla en el aviso del sistema.</p>
</details>

## D√≥nde encontrar otros laboratorios

Puedes explorar laboratorios y ejercicios adicionales en el [Portal de aprendizaje de la Fundici√≥n de IA de Azure](https://ai.azure.com) o consultar la **secci√≥n de laboratorio** del curso para ver otras actividades disponibles.
